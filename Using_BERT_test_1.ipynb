{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "colab_type": "code",
        "id": "fvFvBLJV0Dkv",
        "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datasets\n",
        "#raw_datasets = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zQ-42fh0hjsF"
      },
      "source": [
        "## Importing the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cyoj29J24hPX"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small.jsonl\", lines = True)\n",
        "df_labels = pd.read_json(\"D:\\pan20-authorship-verification-training-small\\pan20-authorship-verification-training-small-truth.jsonl\", lines = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = df['pair'][0][0]\n",
        "text = text[0:13000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def text_segmentate(text, maxlen, seps='\\n', strips=None):\n",
        "    \"\"\"将文本按照标点符号划分为若干个短句\n",
        "    \"\"\"\n",
        "    text = text.strip().strip(strips)\n",
        "    if seps and len(text) > maxlen:\n",
        "        pieces = text.split(seps[0])\n",
        "        text, texts = '', []\n",
        "        for i, p in enumerate(pieces):\n",
        "            if text and p and len(text) + len(p) > maxlen - 1:\n",
        "                texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
        "                text = ''\n",
        "            if i + 1 == len(pieces):\n",
        "                text = text + p\n",
        "            else:\n",
        "                text = text + p + seps[0]\n",
        "        if text:\n",
        "            texts.extend(text_segmentate(text, maxlen, seps[1:], strips))\n",
        "        return texts\n",
        "    else:\n",
        "        return [text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28 46\n",
            "HOHOHO\n",
            "840 46\n"
          ]
        }
      ],
      "source": [
        "datas = []\n",
        "\n",
        "text1 = text_segmentate(text, maxlen=510, seps='.?!')\n",
        "text2 = text_segmentate(df['pair'][23][1], maxlen=510, seps='.?!')\n",
        "\n",
        "print(len(text1), len(text2))\n",
        "\n",
        "while len(text1) < 30 or len(text2) < 30:\n",
        "    print(\"HOHOHO\")\n",
        "    if len(text1) < 30:\n",
        "        n_text1 = []\n",
        "        for i in range(30):\n",
        "            for sent in text1:\n",
        "                n_text1.append(sent)\n",
        "        text1 = n_text1\n",
        "    elif len(text2) < 30:\n",
        "        n_text2 = []\n",
        "        for i in range(30):\n",
        "            for sent in text2:\n",
        "                n_text2.append(sent)\n",
        "        text2 = n_text2\n",
        "datas.append((text1, text2))\n",
        "\n",
        "print(len(text1), len(text2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "487"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(datas[0][0][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>fandoms</th>\n",
              "      <th>pair</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>52596</th>\n",
              "      <td>3926a1bd-6d33-5513-a694-12d7590443d4</td>\n",
              "      <td>[True Blood, Austin &amp; Ally]</td>\n",
              "      <td>[Enjoy! And time stands still beneath the air ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52597</th>\n",
              "      <td>17227d87-a3cb-5aca-b5fd-bb642e88b030</td>\n",
              "      <td>[True Blood, Kuroko no Basuke/黒子のバスケ]</td>\n",
              "      <td>[\"I forgive you,\" she blurted out looking stra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52598</th>\n",
              "      <td>0332dcaa-7230-508b-badb-820f9d8eebaa</td>\n",
              "      <td>[Austin &amp; Ally, Five Nights at Freddy´s]</td>\n",
              "      <td>[\"What? Why? What\"s happening? Where are we go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52599</th>\n",
              "      <td>f5e4884a-13fd-51fb-a880-053d6655d59a</td>\n",
              "      <td>[Five Nights at Freddy´s, Austin &amp; Ally]</td>\n",
              "      <td>[\"Gah!\" I growled. \"I need a password to acces...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52600</th>\n",
              "      <td>900c4a19-e22b-59fd-9cc7-8490919b1696</td>\n",
              "      <td>[Five Nights at Freddy´s, Austin &amp; Ally]</td>\n",
              "      <td>[Chica heard a noise below her and saw Golden ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         id  \\\n",
              "52596  3926a1bd-6d33-5513-a694-12d7590443d4   \n",
              "52597  17227d87-a3cb-5aca-b5fd-bb642e88b030   \n",
              "52598  0332dcaa-7230-508b-badb-820f9d8eebaa   \n",
              "52599  f5e4884a-13fd-51fb-a880-053d6655d59a   \n",
              "52600  900c4a19-e22b-59fd-9cc7-8490919b1696   \n",
              "\n",
              "                                        fandoms  \\\n",
              "52596               [True Blood, Austin & Ally]   \n",
              "52597     [True Blood, Kuroko no Basuke/黒子のバスケ]   \n",
              "52598  [Austin & Ally, Five Nights at Freddy´s]   \n",
              "52599  [Five Nights at Freddy´s, Austin & Ally]   \n",
              "52600  [Five Nights at Freddy´s, Austin & Ally]   \n",
              "\n",
              "                                                    pair  \n",
              "52596  [Enjoy! And time stands still beneath the air ...  \n",
              "52597  [\"I forgive you,\" she blurted out looking stra...  \n",
              "52598  [\"What? Why? What\"s happening? Where are we go...  \n",
              "52599  [\"Gah!\" I growled. \"I need a password to acces...  \n",
              "52600  [Chica heard a noise below her and saw Golden ...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.tail()\n",
        "#df['pair'][0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dMVE3waNhuNj"
      },
      "source": [
        "We take first 2,000 and last 2,000 sentences from the dataset to ensure uniform distribution of labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gTM3hOHW4hUY"
      },
      "outputs": [],
      "source": [
        "batch_1 = pd.concat([df[:2000], df[-2000:]])\n",
        "#batch_1.reset_index(drop=True, inplace=True)\n",
        "batch_1_labels = pd.concat([df_labels[:2000], df_labels[-2000:]])\n",
        "#batch_1_labels.reset_index(drop=True, inplace=True)\n",
        "batch_1_labels['labels'] = batch_1_labels['same'].astype(int) \n",
        "batch_1 = batch_1.join(batch_1_labels['labels'])\n",
        "batch_1 = batch_1.drop(['fandoms', 'id'], axis=1)\n",
        "#batch_1.index = np.arange(0, len(batch_1))\n",
        "batch_1.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load full set without selecting 4000\n",
        "df_labels['labels'] = df_labels['same'].astype(int) \n",
        "batch_1 = df.join(df_labels['labels'])\n",
        "batch_1 = batch_1.drop(['fandoms', 'id'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = datasets.Dataset.from_pandas(batch_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = ppb.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['pair'][0], examples['pair'][1], truncation=True, padding=True, max_length=255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "52601ex [1:09:44, 12.57ex/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset.save_to_disk(r\"C:\\Users\\ivank\\Documents\\BERT_projects\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.remove_columns(['pair'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset['labels'][:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenized_dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset_notest = tokenized_dataset.select(range(47340))\n",
        "tokenized_dataset_fortest = tokenized_dataset.select(range(47340, 52601))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_dataset_tosplit = tokenized_dataset_notest.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_train_dataset = tokenized_dataset_tosplit['train']\n",
        "small_eval_dataset = tokenized_dataset_tosplit['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = ppb.AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = ppb.TrainingArguments('test_trainer', evaluation_strategy=\"epoch\")#, label_names=[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import list_metrics\n",
        "\n",
        "list_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = ppb.Trainer(model=model, args=training_args, \n",
        "train_dataset=small_train_dataset, eval_dataset=small_eval_dataset,\n",
        "compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "100%|██████████| 125/125 [00:04<00:00, 25.56it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.693314254283905,\n",
              " 'eval_accuracy': 0.507,\n",
              " 'eval_runtime': 5.1022,\n",
              " 'eval_samples_per_second': 195.995,\n",
              " 'eval_steps_per_second': 24.499,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 9000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3375\n",
            " 15%|█▍        | 500/3375 [01:12<06:43,  7.12it/s]Saving model checkpoint to test_trainer\\checkpoint-500\n",
            "Configuration saved in test_trainer\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.697, 'learning_rate': 4.259259259259259e-05, 'epoch': 0.44}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-500\\pytorch_model.bin\n",
            " 30%|██▉       | 1000/3375 [02:24<05:32,  7.14it/s]Saving model checkpoint to test_trainer\\checkpoint-1000\n",
            "Configuration saved in test_trainer\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6943, 'learning_rate': 3.518518518518519e-05, 'epoch': 0.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-1000\\pytorch_model.bin\n",
            " 33%|███▎      | 1125/3375 [02:50<05:16,  7.10it/s]  ***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "                                                   \n",
            " 33%|███▎      | 1126/3375 [02:55<1:00:31,  1.61s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.6930590867996216, 'eval_accuracy': 0.507, 'eval_runtime': 4.8886, 'eval_samples_per_second': 204.558, 'eval_steps_per_second': 25.57, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 1500/3375 [03:48<04:28,  6.97it/s]  Saving model checkpoint to test_trainer\\checkpoint-1500\n",
            "Configuration saved in test_trainer\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7028, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-1500\\pytorch_model.bin\n",
            " 59%|█████▉    | 2000/3375 [05:01<03:21,  6.83it/s]Saving model checkpoint to test_trainer\\checkpoint-2000\n",
            "Configuration saved in test_trainer\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7009, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.78}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-2000\\pytorch_model.bin\n",
            " 67%|██████▋   | 2250/3375 [05:39<02:40,  7.01it/s]***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "                                                   \n",
            " 67%|██████▋   | 2251/3375 [05:44<30:59,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.693905770778656, 'eval_accuracy': 0.493, 'eval_runtime': 5.0241, 'eval_samples_per_second': 199.039, 'eval_steps_per_second': 24.88, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 2500/3375 [06:19<02:05,  6.95it/s]Saving model checkpoint to test_trainer\\checkpoint-2500\n",
            "Configuration saved in test_trainer\\checkpoint-2500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7006, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-2500\\pytorch_model.bin\n",
            " 89%|████████▉ | 3000/3375 [07:32<00:53,  6.97it/s]Saving model checkpoint to test_trainer\\checkpoint-3000\n",
            "Configuration saved in test_trainer\\checkpoint-3000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7003, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in test_trainer\\checkpoint-3000\\pytorch_model.bin\n",
            "100%|██████████| 3375/3375 [08:28<00:00,  7.18it/s]***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "                                                   \n",
            "100%|██████████| 3375/3375 [08:33<00:00,  7.18it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 3375/3375 [08:33<00:00,  6.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.693314254283905, 'eval_accuracy': 0.507, 'eval_runtime': 5.0181, 'eval_samples_per_second': 199.277, 'eval_steps_per_second': 24.91, 'epoch': 3.0}\n",
            "{'train_runtime': 513.3337, 'train_samples_per_second': 52.597, 'train_steps_per_second': 6.575, 'train_loss': 0.6994329517505787, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3375, training_loss=0.6994329517505787, metrics={'train_runtime': 513.3337, 'train_samples_per_second': 52.597, 'train_steps_per_second': 6.575, 'train_loss': 0.6994329517505787, 'epoch': 3.0})"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions = predictions, references = labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#del model\n",
        "#del trainer\n",
        "#del pytorch_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_train_dataset.set_format('torch')\n",
        "small_eval_dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=10)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ppb.AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = ppb.AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs=5\n",
        "num_training_steps=num_epochs*len(train_dataloader)\n",
        "lr_scheduler = ppb.get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21305/21305 [1:01:55<00:00,  6.21it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'f1': 0.7979550102249489}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"f1\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "for p, pair in enumerate(batch_1['pair']):\n",
        "    #for i, text in enumerate(pair):\n",
        "    #    pair[i] = text[0:250]\n",
        "    batch_1['pair'][p] = ' [SEP] '.join(pair)\n",
        "\"\"\"\n",
        "# code for splitting before tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PRc2L89hh1Tf"
      },
      "source": [
        "We check how many pairs are labeled as `same` (value 1) and how many are labeled `different` (having the value 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "jGvcfcCP5xpZ",
        "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    2000\n",
              "0    2000\n",
              "Name: same, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_1_labels['same'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_MO08_KiAOb"
      },
      "source": [
        "## Loading the Pre-trained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "q1InADgf5xm2",
        "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ppb' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36080/2094719149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# BERT:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mppb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mppb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Load pretrained model/tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ppb' is not defined"
          ]
        }
      ],
      "source": [
        "# DistilBERT:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "# BERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "# Avoid unnecessary warnings\n",
        "ppb.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lZDBMn3wiSX6"
      },
      "source": [
        "The variable `model` holds a pretrained BERT model.\n",
        "\n",
        "## Model #1: Preprocessing and Tokenization\n",
        "\n",
        "Tokenization is performed using BertTokenizer and may take 7-10 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized = batch_1['pair'].apply((lambda x: tokenizer(text = x[0], text_pair = x[1],\n",
        "add_special_tokens=True, truncation=True, max_length=201, padding=True, return_tensors=\"pt\").to(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  5670,  1037,  2978,  1010, 23625,  5599,  2026,  2159,\n",
              "         14957,  2013,  2028, 13547,  2000,  1996,  2060,  1011,  1011,  2021,\n",
              "          2026,  2159,  2024,  4738,  2006,  1996,  8659, 13547,  1996,  2087,\n",
              "          1012,  2066,  7570,  9890,  1012,  1012,  1012,  2061,  2066,  7570,\n",
              "          9890,  1012,  1012,  1012,  2002,  4332,  1037,  2978,  1010,  1998,\n",
              "          2256,  2159,  3113,  3495,  1012,  1045,  2064,  1000,  1056,  6235,\n",
              "          2009,  1012,  1012,  1012,  1999,  2023,  2279,  2617,  1010,  1045,\n",
              "          2123,  1000,  1056,  2298,  2185,  1010,  2129,  9596,  2009,  3849,\n",
              "          1012,  1045,  6237,  2046,  2010,  2159,  1012,  2027,  1000,  2128,\n",
              "          2066,  7570,  9890,  1000,  1055,  1012,  1012,  1012,  2027,  2024,\n",
              "           102,  1000,  2035,  2097,  2468,  2028,  2007,  3607,  1010,  1000,\n",
              "          2002,  2056,  1010,  2471,  3432,  1010,  2010, 15138, 18823,  1012,\n",
              "         10006,  2020,  2525,  8555,  1025,  2085,  2027, 24665, 24174,  2055,\n",
              "          1010,  2005,  1037,  6090,  1010,  1037,  5883,  1010,  1037,  4690,\n",
              "          1011,  2045,  2001,  2498,  1012,  1999,  2070,  2126,  1010,  2023,\n",
              "          2716,  2014,  2021,  1037,  6682,  1997,  4335,  1011,  7664,  1998,\n",
              "          8469, 13149,  1010,  2016,  2001,  6966,  1010,  2020,  2025,  2182,\n",
              "          2000,  9015,  2004,  2092,  1012,  2065,  7332,  2404,  2010,  5016,\n",
              "          2398,  2006,  8469, 13149,  1012,  1012,  1012, 11562,  1010,  2253,\n",
              "          2019,  4874,  1010,  1998, 13234, 19510,  2050,  2001,  5941,  2046,\n",
              "           102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized[0] #let's check what the tokenized input looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tokenized.index = np.arange(0, len(tokenized))\n",
        "#tokenized = tokenized.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#re-indexing was required for some experiments, but not amymore. Use re-indexing when you iterate over enumerated rows. \n",
        "\n",
        "#tokenized_index[1999]\n",
        "#tokenized_index[2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "for i, pair in enumerate(tokenized):\n",
        "    #input_id = tokenized[pair]['input_ids']\n",
        "    #tokenized[i] = ' [SEP] '.join(pair['input_ids'])\n",
        "    #if i == 0:\n",
        "    #    print(i, \"PPAAAAIR\", pair)\n",
        "    #    print(\"000\", tokenized[i]['input_ids'])\n",
        "    #print(i)\n",
        "    texts = tokenized[i]['input_ids'][0] + tokenized[i]['input_ids'][1]\n",
        "    types = tokenized[i]['token_type_ids'][0] + tokenized[i]['token_type_ids'][1]\n",
        "    masks = tokenized[i]['attention_mask'][0] + tokenized[i]['attention_mask'][1]\n",
        "    if i > 1999:\n",
        "        #print(text)\n",
        "        tokenized[i]['input_ids'] = texts\n",
        "        tokenized[i]['token_type_ids'] = types\n",
        "        tokenized[i]['attention_mask'] = masks\n",
        "\"\"\"        \n",
        "\n",
        "# code for splitting after tokenization. Not used in batched processing.\\\n",
        "\n",
        "\"\"\"\n",
        "token_tensors = []\n",
        "for i, pair in enumerate(tokenized):\n",
        "    token_tensors.append(pair['input_ids'])\n",
        "\n",
        "token_tensors_df = pd.DataFrame(token_tensors)\n",
        "\"\"\"\n",
        "\n",
        "# code for extracting the encodings. Not used in batched processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "URn-DWJt5xhP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\"\"\"\n",
        "# code for padding when it is not performed during tokenization\n",
        "\n",
        "# to simulate padding:\n",
        "# padded = np.array([i for i in token_tensors_df.values])\n",
        "\n",
        "#np.array(padded).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "4K_iGRNa_Ozc",
        "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape\n",
        "\"\"\"\n",
        "\n",
        "# adding attention mask if it is not performed during tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jK-CQB9-kN99"
      },
      "source": [
        "## Model #1 Learning\n",
        "\n",
        "The `model()` function runs our sentences through BERT (it is inherited by BERT's `forward()` function). The results of the processing will be returned into `all_output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "39UVjAV56PJz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "print(device)\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "\"\"\"\n",
        "\n",
        "# standard training with separate, manually created encodings and attention masks (no batching => may require a lot of memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test model to try different modes \n",
        "\n",
        "test_input = tokenizer([\"Hello, my dog is cute\", \"And what now?\"], padding=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "type(test_input)\n",
        "model = model.to(\"cuda:0\")\n",
        "test_output = model(**test_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4000, 40, 768])\n"
          ]
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "\n",
        "all_outputs = torch.Tensor()\n",
        "all_outputs = all_outputs.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tokenized:\n",
        "        \n",
        "        #batch['input_ids'].to(device)              # not needed if CUDA was enabled during tokenization\n",
        "        #batch['token_type_ids'].to(device)\n",
        "        #batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        all_outputs = torch.cat((all_outputs, outputs.last_hidden_state), 0)\n",
        "    print(all_outputs.shape)       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized.index = np.arange(0, len(tokenized))\n",
        "# tokenized.reset_index()#drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27108/1109291229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             raise KeyError(\n\u001b[0m\u001b[0;32m    244\u001b[0m                 \u001b[1;34m\"Indexing with integers (to access backend Encoding for a given batch index) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[1;34m\"is not available when using Python based tokenizers\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Indexing with integers (to access backend Encoding for a given batch index) is not available when using Python based tokenizers'"
          ]
        }
      ],
      "source": [
        "tokenized[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataset = torch.utils.data.TensorDataset(tokenized, torch.Tensor(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "#model = model.to(device)\n",
        "\n",
        "#with torch.no_grad():\n",
        "#    features = tokenized.apply(lambda x: model(**x).last_hidden_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoCep_WVuB3v"
      },
      "source": [
        "Extract `[CLS]` tokens to used them as input to simple LogReg classifier and save them in the `features` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = all_outputs[:,0,:].cpu().numpy() # [:,0,:] meaning: ':' for all sequences, '0' for first token in sequence, ':' for all 768 hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_VZVU66Gurr-"
      },
      "source": [
        "The labels indicating which pair is written by the same author or by different ones go into the `labels` variable\n",
        "\n",
        "We also randomly permute labels and then permute features according to the same pattern. This is done to provide uniform distribution of lables after train/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JD3fX2yh6PTx"
      },
      "outputs": [],
      "source": [
        "labels = batch_1_labels['same']\n",
        "\n",
        "idx = np.random.permutation(labels.index)\n",
        "labels = labels.reindex(idx)\n",
        "features = pd.DataFrame(features)\n",
        "features = features.reindex(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TensorDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27108/2387714313.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset = TensorDataset(torch.Tensor(features), torch.Tensor(labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iaoEvM2evRx1"
      },
      "source": [
        "## Model #2: Train/Test Split\n",
        "We now split the obtained datset into a training set and testing set in standard proportion (75% to 25%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ddAqbkoU6PP9"
      },
      "outputs": [],
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cyEwr7yYD3Ci"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best parameters:  {'C': 100.0}\n",
            "best scores:  0.6076666666666667\n"
          ]
        }
      ],
      "source": [
        "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
        "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
        "grid_search.fit(train_features, train_labels)\n",
        "\n",
        "print('best parameters: ', grid_search.best_params_)\n",
        "print('best scores: ', grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "colab_type": "code",
        "id": "gG-EVWx4CzBc",
        "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=97)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr_clf = LogisticRegression(C=97)\n",
        "lr_clf.fit(train_features, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "iCoyxRJ7ECTA",
        "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.603"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr_clf.score(test_features, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "lnwgmqNG7i5l",
        "outputId": "0042aed2-4fa8-4fa0-bf25-fdef70a10aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy classifier score: 0.503 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier()\n",
        "\n",
        "scores = cross_val_score(clf, train_features, train_labels)\n",
        "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EJQuqV6cnWQu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "A Visual Notebook to Using BERT for the First Time.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
